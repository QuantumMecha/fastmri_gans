{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1638888879292,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "GqM5CVn47CiA",
    "outputId": "6722b794-587e-45f4-8f6f-59a3aa5eaffe"
   },
   "outputs": [],
   "source": [
    "# ### Run this cell only if using Colab... ###\n",
    "# # Mount the Google Drive so we can access data\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/CS7643_Project/Code/isaac/Transformer_256/')\n",
    "\n",
    "# basefolder = '/content/drive/My Drive/CS7643_Project/Data/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1638888880888,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "m7gA4veYMeCQ"
   },
   "outputs": [],
   "source": [
    "# ## Uncomment and run this cell if NOT using Colab... ###\n",
    "# basefolder = '../../../data/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638888880889,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "Mdr5gbIfbRLs",
    "outputId": "bd64a8de-92e9-42e9-fff8-236fad6d73a5"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638888880890,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "7WqXHQJBb9p5",
    "outputId": "af5e763b-58e0-4e88-952e-c9cbf4eb2ab7"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4179,
     "status": "ok",
     "timestamp": 1638888885062,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "PcavZy43biIE",
    "outputId": "95d2fa59-48fa-42ed-99f0-336975ddb60b"
   },
   "outputs": [],
   "source": [
    "# %pip install -r '/content/drive/My Drive/CS7643_Project/Code/isaac/Transformer_256/requirements.txt' # For transformer\n",
    "%pip install -r 'requirements.txt' # For transformer\n",
    "%pip install s3fs\n",
    "# %pip install h5py\n",
    "# %pip install scikit-image\n",
    "# %pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3421,
     "status": "ok",
     "timestamp": 1638888888475,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "2eRIX-4pViRC"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.utils import save_image\n",
    "from skimage.transform import rescale\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import h5py\n",
    "import gc\n",
    "import s3fs\n",
    "\n",
    "import pytorch_ssim\n",
    "import unet_gan\n",
    "import transformer\n",
    "\n",
    "from vgg_loss import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQaRhxYj65PE"
   },
   "source": [
    "# Set some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638888888477,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "-sEobQzR65PH",
    "outputId": "46bcfef1-51b1-4f24-8dd4-e1174b8af9a7"
   },
   "outputs": [],
   "source": [
    "num_classes = 2  # Number of classes for the discriminator, which should be 2 since it's a binary classification problem\n",
    "img_size = 300  # Number of pixels for h & w\n",
    "batch_size = 16\n",
    "lr = 0.0002  # Learning rate, in DCGAN paper it is 0.0002\n",
    "ngpu = 1 # Number of GPUs\n",
    "# Decide which device we want to run on\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XuOF9WE65PK"
   },
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638888888479,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "eA01swn7NqxP"
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "s3.ls('s3://cs7643-fastmri/data/preprocessed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDVBRF1CRkJD"
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "executionInfo": {
     "elapsed": 136569,
     "status": "ok",
     "timestamp": 1638889025040,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "9R4wyqtn65PL",
    "outputId": "11f0c9dd-d098-49ce-89aa-bd0b3d2c1ef8"
   },
   "outputs": [],
   "source": [
    "# Get the unsubsampled training data\n",
    "# hff = h5py.File(basefolder + 'brain_AXFLAIR_full_TRAIN_1637456991.970846.h5', 'r')\n",
    "# train_full = np.array(hff['full_train'])\n",
    "filename = 's3://cs7643-fastmri/data/preprocessed/brain_AXFLAIR_full_TRAIN_1637456991.970846.h5'\n",
    "hff = h5py.File(s3.open(filename, 'rb'), 'r')\n",
    "train_full = np.array(hff['full_train'])\n",
    "print(train_full.shape)\n",
    "\n",
    "# rescale instead of cropping to size\n",
    "# train_full = np.array([rescale(img, 0.213, anti_aliasing=True) for img in train_full]) # rescaled to 64*64\n",
    "train_full = np.array([rescale(img, 0.4266, anti_aliasing=True) for img in train_full]) # rescaled to 128*128\n",
    "print(np.max(train_full))\n",
    "\n",
    "# Crop to 288 x 288 -- the Unet needs to have the image size in multiples of 32!!!\n",
    "# train_full = train_full[:2,6:(300-6), 6:(300-6)] # 288 * 288\n",
    "# train_full = train_full[:,22:(300-22), 22:(300-22)] # 256 * 256\n",
    "# train_full = train_full[:,86:(300-86), 86:(300-86)] # 128 * 128\n",
    "# train_full = train_full[:,118:(300-118), 118:(300-118)] # 64 * 64\n",
    "print(train_full.shape)\n",
    "\n",
    "# Show an example\n",
    "#plt.imshow(train_full[2843], cmap='gray')\n",
    "plt.imshow(train_full[1], cmap='gray')\n",
    "\n",
    "# Make it a pytorch tensor\n",
    "train_full = train_full * (1.0 / np.max(train_full))\n",
    "train_full = torch.from_numpy(train_full).unsqueeze(1)\n",
    "print(train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 158405,
     "status": "ok",
     "timestamp": 1638889183435,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "ujf1OKQI65PM",
    "outputId": "afd82505-2738-48e8-8a80-0c8a388832c9"
   },
   "outputs": [],
   "source": [
    "# Get the subsampled training data\n",
    "# hff = h5py.File(basefolder + 'brain_AXFLAIR_subsampled_TRAIN_1637456991.970846.h5', 'r')\n",
    "# train_ss = np.array(hff['subsample_train'])\n",
    "filename = 's3://cs7643-fastmri/data/preprocessed/brain_AXFLAIR_subsampled_TRAIN_1637456991.970846.h5'\n",
    "hff = h5py.File(s3.open(filename, 'rb'), 'r')\n",
    "train_ss = np.array(hff['subsample_train'])\n",
    "print(train_ss.shape)\n",
    "\n",
    "# rescale instead of cropping to size\n",
    "# train_ss = np.array([rescale(img, 0.213, anti_aliasing=True) for img in train_ss]) # rescaled to 64*64\n",
    "train_ss = np.array([rescale(img, 0.4266, anti_aliasing=True) for img in train_ss]) # rescaled to 128*128\n",
    "\n",
    "# Crop to 288 x 288 -- the Unet needs to have the image size in multiples of 32!!!\n",
    "# train_ss = train_ss[:2,6:(300-6), 6:(300-6)] # 288 * 288\n",
    "# train_ss = train_ss[:,22:(300-22), 22:(300-22)] # 256 * 256\n",
    "# train_ss = train_ss[:,86:(300-86), 86:(300-86)] # 128 * 128\n",
    "# train_ss = train_ss[:,118:(300-118), 118:(300-118)] # 64 * 64\n",
    "print(train_ss.shape)\n",
    "\n",
    "# Show an example\n",
    "#plt.imshow(train_ss[2843], cmap='gray')\n",
    "plt.imshow(train_ss[1], cmap='gray')\n",
    "\n",
    "# Make it a pytorch tensor\n",
    "train_ss = train_ss * (1.0 / np.max(train_ss))\n",
    "train_ss = torch.from_numpy(train_ss).unsqueeze(1)\n",
    "print(train_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove low quality images from data sets\n",
    "print(train_full.shape)\n",
    "print(train_ss.shape)\n",
    "\n",
    "for removal_index, index in enumerate([14,15,29,43,44,45,60,61,76,77,91,92,93,108,109,122,123,124,125,139,140,141,156,157,172,173,187,188,189,204,205,221,235,236,237,249,\n",
    "              250,251,252,253,267,268,269,284,285,300,301,317,331,332,333,348,349,363,364,365,379,380,381,396,397,411,412,413,427,428,429,444,445,\n",
    "              460,461,474,475,476,477,491,492,493,508,509,523,524,525,540,541,557,573,589,605,619,634,635,651,667,683,698,699,715,731,747,763,777,\n",
    "              778,779,795,826,827,843,859,875,906,907,923,939,955,970,971,986,987,1003,1034,1035,1050,1051,1067,1083,1115,1130,1131,1160,1161,1177,\n",
    "              1191,1223,1255,1271,1286,1287,1303,1319,1335,1350,1351,1367,1383,1399,1415,1430,1430,1431,1446,1447,1462,1463,1509,1523,1551,1565,1566,\n",
    "              1567,1583,1643,1689,1753,1769,1785,1801,1814,1815,1831,1862,1863,1909,1910,1911,1927,1943,1959,2007,2023,2039,2054,2055,2133,2144,2145,\n",
    "              2160,2161,2175,2176,2177,2192,2193,2240,2241,2273,2288,2289,2305,2321,2336,2337,2352,2353,2368,2369,2415,2431,2445,2446,2447,2463,2479,\n",
    "              2494,2495,2543,2559,2574,2575,2591,2606,2607,2638,2639,2655,2670,2671,2687,2701,2702,2703,2734,2735,2751,2766,2767,2783,2797,2799,2815,\n",
    "              2828,2829,2830,2831,2846,2847,2861,2862,2863,2879,2895,2911,2926,2927,2943,2958,2959,2991,3022,3023,3055,3069,3070,3071,3087,3102,3103,\n",
    "              3118,3119,3167,3082,3183,3198,3199,3214,3215,3230,3231,3246,3247,3263,3778,3279,3295,3311,3324,3325,3340,3341,3371,3387,3402,3403,3419,\n",
    "              3450,3451,3467,3483,3498,3499,3513,3514,3515,3531,3547,3562,3563,3611,3627,3643,3659,3674,3675,3691,3707,3721,3721,3722,3723,3755,3771,\n",
    "              3787,3802,3803,3819,3834,3835,3851,3867,3882,3883,3899,3913,3928,3929,3943,3958,3959,3974,3975,3989,3990,3991,4006,4007,4023,4038,4039,\n",
    "              4055,4069,4084,4085,4099,4115,4130,4131,4146,4147,4160,4161,4176,4177,4193,4207,4208,4209,4223,4224,4225,4241,4256,4257,4273,4287,4288,\n",
    "              4289,4304,4305,4320,4321,4336,4337,4353,4369,4384,4385,4400,4401,4415,4416,4417,4433,4448,4449,4464,4465,4480,4481,4496,4497,4513,4529,\n",
    "              4545,4561,4576,4577,4591,4592,4593,4607,4608,4609,4623,4624,4625,4641,4657,4673,4686,4687,4688,4705,4718,4719,4720,4721,4736,4737,4751,\n",
    "              4752,4753]):\n",
    "    train_full = np.delete(train_full, index-removal_index, axis=0)\n",
    "    train_ss = np.delete(train_ss, index-removal_index, axis=0)\n",
    "    \n",
    "print(train_full.shape)\n",
    "print(train_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "# for img in train_full:\n",
    "#     plt.imshow(img.squeeze().cpu().detach().numpy(), cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1638889183437,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "N7KJrcPw65PN"
   },
   "outputs": [],
   "source": [
    "# Since we want to iterate over two datasets simultaneously (full and subsampled), we combine them into one (...for index, (xb1, xb2) in enumerate(dataloader):...)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_full, train_ss)\n",
    "dataloader1 = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Train datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-97jYeuzRea1"
   },
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1638889183438,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "v_jCzrmh65PO"
   },
   "outputs": [],
   "source": [
    "# Get the unsubsampled validation data\n",
    "# hff = h5py.File(basefolder+'brain_AXFLAIR_full_VAL_1637484616.411167.h5', 'r')\n",
    "# val_full = np.array(hff['full_val'])\n",
    "filename = 's3://cs7643-fastmri/data/preprocessed/brain_AXFLAIR_full_VAL_1637484616.411167.h5'\n",
    "hff = h5py.File(s3.open(filename, 'rb'), 'r')\n",
    "val_full = np.array(hff['full_val'])\n",
    "print(val_full.shape)\n",
    "\n",
    "val_full = np.array([rescale(img, 0.4266, anti_aliasing=True) for img in val_full]) # rescaled to 128*128\n",
    "\n",
    "# Crop to 288 x 288 -- the Unet needs to have the image size in multiples of 32!!!\n",
    "# val_full = val_full[:,6:(300-6), 6:(300-6)]\n",
    "# print(val_full.shape)\n",
    "\n",
    "# Show an example\n",
    "plt.imshow(val_full[10], cmap='gray')\n",
    "\n",
    "# Make it a pytorch tensor\n",
    "val_full = val_full * (1.0 / np.max(val_full))\n",
    "val_full = torch.from_numpy(val_full).unsqueeze(1)\n",
    "val_full.to('cpu')\n",
    "print(val_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1638889183439,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "8dAJlw2k65PQ"
   },
   "outputs": [],
   "source": [
    "# Get the subsampled validation data\n",
    "# hff = h5py.File(basefolder+'brain_AXFLAIR_subsampled_VAL_1637484616.411167.h5', 'r')\n",
    "# val_ss = np.array(hff['subsample_val'])\n",
    "filename = 's3://cs7643-fastmri/data/preprocessed/brain_AXFLAIR_subsampled_VAL_1637484616.411167.h5'\n",
    "hff = h5py.File(s3.open(filename, 'rb'), 'r')\n",
    "val_ss = np.array(hff['subsample_val'])\n",
    "print(val_ss.shape)\n",
    "\n",
    "val_ss = np.array([rescale(img, 0.4266, anti_aliasing=True) for img in val_ss]) # rescaled to 128*128\n",
    "\n",
    "# Crop to 288 x 288 -- the Unet needs to have the image size in multiples of 32!!!\n",
    "# val_ss = val_ss[:,6:(300-6), 6:(300-6)]\n",
    "# print(val_ss.shape)\n",
    "\n",
    "# Show an example\n",
    "plt.imshow(val_ss[10], cmap='gray')\n",
    "\n",
    "# Make it a pytorch tensor\n",
    "val_ss = val_ss * (1.0 / np.max(val_ss))\n",
    "val_ss = torch.from_numpy(val_ss).unsqueeze(1)\n",
    "val_ss.to('cpu')\n",
    "print(val_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove low quality images from data sets\n",
    "print(val_full.shape)\n",
    "print(val_ss.shape)\n",
    "\n",
    "for removal_index, index in enumerate([14,15,30,31,45,46,47,62,63,78,79,92,93,94,95,110,111,125,126,127,142,143,189,159,174,175,190,191,205,219,220,221,\n",
    "                                       236,237,250,251,266,267,282,283,298,299,312,313,314,315,329,330,331,346,347,362,363,378,379,393,394,395,409,410,\n",
    "                                       411,425,426,427,441,442,443,458,459,474,475,489,490,491,505,506,507,521,522,523,537,538,539,554,555,571,601,615,\n",
    "                                       616,617,630,631,632,633,645,646,647,648,649,662,663,664,665,680,681,695,696,697,712,713,728,729,745,759,760,761,777,\n",
    "                                       792,793,808,809,823,824,825,839,840,841,856,857,871,872,873,888,889,920,921,935,936,937,951,952,953,967,968,969,980,\n",
    "                                       981,982,983,984,985,1000,1001,1016,1017,1030,1031,1046,1047,1062,1063,1077,1078,1079,1093,1094,1095,1109,1110,1111,1125,\n",
    "                                       1126,1127,1141,1142,1143,1157,1158,1159,1174,1175,1188,1189,1200,1201,1216,1217,1232,1233,1249,1264,1265,1279,1280,1281,\n",
    "                                       1295,1296,1297,1312,1313,1327,1328,1329,1344,1345,1357,1358,1359,1360,1361,1375,1376,1377,1389,1390,1391,1392,1393,1408,\n",
    "                                       1409,1423,1424,1425,1438,1439,1454,1455,1469,1470,1471,1485,1486,1487,1501,1502,1503,1518,1519,1534,1535,1550,1551,1565,\n",
    "                                       1566,1567,1580,1581,1596,1597,1611,1612,1613,1627,1628,1629,1644,1645,1659,1660,1661,1676,1677,1690,1691,1692,1693]):\n",
    "    val_full = np.delete(val_full, index-removal_index, axis=0)\n",
    "    val_ss = np.delete(val_ss, index-removal_index, axis=0)\n",
    "    \n",
    "print(val_full.shape)\n",
    "print(val_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1638889183441,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "Mqu_hxFQ65PR"
   },
   "outputs": [],
   "source": [
    "# Same with the validation data (full and subsampled), we combine them into one (...for index, (vf, vs) in enumerate(dataloader):...)\n",
    "# Since we want to iterate over two datasets simultaneously (full and subsampled), we combine them into one (...for index, (xb1, xb2) in enumerate(dataloader):...)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_full, val_ss)\n",
    "dataloader2 = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BkrxRmX65PS"
   },
   "source": [
    "# Set up the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1638889183441,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "Ncn3xke865PT"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD (per original DCGAN paper)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889183442,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "F06wSlatmhv_"
   },
   "outputs": [],
   "source": [
    "# Feature extractor, we need this to compute content loss\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6242,
     "status": "ok",
     "timestamp": 1638889189669,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "FRajJW9dli4X",
    "outputId": "93cab2b5-5e96-40ee-a13d-c771b0b36541"
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = transformer.Discriminator(diff_aug=\"translation\",d_depth=3,d_act=\"gelu\",d_norm=None,df_dim=384, d_window_size=4, img_size=128, patch_size=8, in_chans=1, num_classes=1, depth=7,\n",
    "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0.5, attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer='ln')\n",
    "# D = unet_gan.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "# to mean=0, stdev=0.02.\n",
    "D.apply(weights_init)\n",
    "D.to(device)\n",
    "print(device)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2174,
     "status": "ok",
     "timestamp": 1638889191835,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "gggPTHxQ65PV",
    "outputId": "5fb49d5d-2433-45b0-f46f-74e718880bed"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "G = transformer.Generator(g_act=\"gelu\", mlp_ratio=4, drop_rate=0.5, img_size=128, patch_size=8, in_chans=1,embed_dim=384, depth=5,num_heads=4, qkv_bias=False, qk_scale=None, attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.5, hybrid_backbone=None, norm_layer=nn.LayerNorm)#,device = device)\n",
    "# G = unet_gan.Generator(ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "# to mean=0, stdev=0.02.\n",
    "G.apply(weights_init)\n",
    "G.to(device)\n",
    "print(device)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1638889191836,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "20rbCpDDm-rv"
   },
   "outputs": [],
   "source": [
    "# Initialize the feature extractor - this is for the VGG loss\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4fCujhiuRUN"
   },
   "source": [
    "## Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3FVhVp4bkKj"
   },
   "source": [
    "### Discriminator Loss - we will use Wasserstein Loss.  See here for more info:\n",
    "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1638889191837,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "R4x3ZnNyuQhK"
   },
   "outputs": [],
   "source": [
    "# Wasserstein Loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "\treturn torch.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1638889191838,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "gCh8wjSTs-Vb"
   },
   "outputs": [],
   "source": [
    "# FFT MSE Loss (per Yang, et al)\n",
    "def fft_mse(y_true, y_pred):\n",
    "  y_true_fft = torch.fft.fftn(y_true)\n",
    "  y_pred_fft = torch.fft.fftn(y_pred)\n",
    "  return criterion_mse(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889191839,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "zwNrgsFyBBDU"
   },
   "outputs": [],
   "source": [
    "# FFT L1 Loss (per Yang, et al)\n",
    "def fft_l1(y_true, y_pred):\n",
    "  y_true_fft = torch.fft.fftn(y_true)\n",
    "  y_pred_fft = torch.fft.fftn(y_pred)\n",
    "  return criterion_content(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1638889191839,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "Nnret5ij3ntF"
   },
   "outputs": [],
   "source": [
    "# Total Variational Loss\n",
    "def tv_loss(img, tv_weight):\n",
    "    \"\"\"\n",
    "    Compute total variation loss.\n",
    "    Inputs:\n",
    "    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.\n",
    "    - tv_weight: Scalar giving the weight w_t to use for the TV loss.\n",
    "    Returns:\n",
    "    - loss: PyTorch Variable holding a scalar giving the total variation loss\n",
    "      for img weighted by tv_weight.\n",
    "    \"\"\"\n",
    "    w_variance = torch.sum(torch.pow(img[:,:,:,:-1] - img[:,:,:,1:], 2))\n",
    "    h_variance = torch.sum(torch.pow(img[:,:,:-1,:] - img[:,:,1:,:], 2))\n",
    "    loss = tv_weight * (h_variance + w_variance)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889191840,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "JT2IVpAElXbp"
   },
   "outputs": [],
   "source": [
    "# Losses per the original super resolution paper\n",
    "criterion_mse = torch.nn.MSELoss().to(device)\n",
    "criterion_content = torch.nn.L1Loss().to(device)\n",
    "\n",
    "criterion_ssim = pytorch_ssim.SSIM().to(device)\n",
    "criterion_wasserstein = wasserstein_loss\n",
    "\n",
    "def calc_ssim_loss(y_true, y_pred):\n",
    "  return 1 - criterion_ssim(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889191841,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "goBMEDaEmCXo"
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889191841,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "fIIZIjSNqo8j"
   },
   "outputs": [],
   "source": [
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "  os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638889191842,
     "user": {
      "displayName": "isaac taylor",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00640694701919560636"
     },
     "user_tz": -630
    },
    "id": "j4uKiOCF6BcT"
   },
   "outputs": [],
   "source": [
    "# # setup tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52NEqSu2m9fk",
    "outputId": "2d3f4a5c-9b50-4057-b709-154db38d381a"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "clip_value = 0.01 # For clipping the weights of the discriminator to enforce the Lipschitz constraint - see https://theaisummer.com/gan-computer-vision-incremental-training/\n",
    "\n",
    "# Losses by epoch\n",
    "d_losses = []  # Training\n",
    "g_losses = []  # Training\n",
    "dv_losses = [] # Validation\n",
    "gv_losses = [] # Validation\n",
    "ssim_v_losses = [] # Validation\n",
    "\n",
    "# Losses by batch\n",
    "dv_lossesb = [] # Validation\n",
    "gv_lossesb = [] # Validation\n",
    "ssim_v_lossesb = [] # Validation\n",
    "\n",
    "best_avg_g_loss = 1000000\n",
    "\n",
    "for epoch in range(20):\n",
    "  d_lossesb = []  # Training\n",
    "  g_lossesb = []  # Training\n",
    "  for index, (tf, ts) in enumerate(dataloader1):\n",
    "    #print('input shapes', tf.shape, ts.shape)  # Debug\n",
    "\n",
    "    tf = tf.float().to(device)\n",
    "    ts = ts.float().to(device)\n",
    "    \n",
    "    unshaped_ts = ts[0,0].detach().clone()\n",
    "    ts = ts.reshape(ts.shape[0], (ts.shape[1] * ts.shape[2] * ts.shape[3]))\n",
    "    \n",
    "    valid = torch.ones(tf.shape[0]).float().to(device) # Labels\n",
    "    fake = torch.zeros(tf.shape[0]).float().to(device) # Labels\n",
    "\n",
    "    # Generate a high resolution image from low resolution input\n",
    "    fake_images = G(ts)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "    D.zero_grad()\n",
    "\n",
    "    # Total loss = mean of real loss + mean of fake loss (Wasserstein)\n",
    "    d_loss = -torch.mean(D(tf)) + torch.mean(D(fake_images.detach()))\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    # Clip weights of discriminator\n",
    "    for p in D.parameters():\n",
    "      p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "    # ------------------\n",
    "    #  Train Generator\n",
    "    # ------------------\n",
    "    # Note: The original SR GAN paper has the following losses for the generator:\n",
    "    #       1. MSE_loss (Between real and generated)\n",
    "    #       2. VGG_loss (Using the VGG)\n",
    "    #       3. Adversarial_loss (BCE, but could be Wasserstein)\n",
    "    G.zero_grad()\n",
    "\n",
    "    # Adversarial loss\n",
    "    adv_loss = criterion_content(D(fake_images), valid)\n",
    "    #adv_loss = criterion_mse(D(fake_images), valid)  # For MSE Loss, from Erik Lindernoren SRGAN implementation\n",
    "    if adv_loss < 0: print(f\"adv_loss: {adv_loss}\")\n",
    "    adv_loss = torch.clamp(adv_loss, min=0, max=None)  # For L1 Loss (Best)\n",
    "\n",
    "    # Content loss\n",
    "    # Get the pixel-by-pixel loss\n",
    "    mse_loss = criterion_mse(fake_images, tf)\n",
    "    if mse_loss < 0: print(f\"mse_loss: {mse_loss}\")\n",
    "    mse_loss = torch.clamp(mse_loss, min=0, max=None) # L2 MSE Loss\n",
    "    \n",
    "    l1_loss = criterion_content(fake_images, tf)\n",
    "    if l1_loss < 0: print(f\"l1_loss: {l1_loss}\")\n",
    "    l1_loss = torch.clamp(l1_loss, min=0, max=None) #L1 MAE Loss\n",
    "\n",
    "    # FFT loss - the pixel-by-pixel loss in the frequency domain\n",
    "    #fft_loss = fft_mse(fake_images, tf) # L2 MSE Loss    \n",
    "    fft_loss = fft_l1(fake_images, tf) #L1 MAE Loss\n",
    "    if fft_loss < 0: print(f\"fft_loss: {fft_loss}\")\n",
    "    fft_loss = torch.clamp(fft_loss, min=0, max=None) \n",
    "\n",
    "\n",
    "    # Perceptual loss (VGG)\n",
    "    # Since the VGG19 take in 3-channel images, we need to concatenate\n",
    "    fake_images_3ch = torch.cat((fake_images, fake_images, fake_images), dim=1)\n",
    "    real_images_3ch = torch.cat((tf, tf, tf), dim=1)\n",
    "    gen_features = feature_extractor(fake_images_3ch)\n",
    "    real_features = feature_extractor(real_images_3ch)\n",
    "    \n",
    "    vgg_loss = criterion_content(gen_features, real_features) # L1 Loss\n",
    "    #vgg_loss = criterion_mse(gen_features, real_features) # L2 loss\n",
    "    if vgg_loss < 0: print(f\"vgg_loss: {vgg_loss}\")\n",
    "    vgg_loss = torch.clamp(vgg_loss, min=0, max=None)\n",
    "\n",
    "    # SSIM loss\n",
    "    ssim_loss = criterion_ssim(fake_images, tf)\n",
    "    if ssim_loss < 0: print(f\"vgg_loss: {ssim_loss}\")\n",
    "    ssim_loss = torch.clamp(ssim_loss, min=0, max=None)\n",
    "\n",
    "    # Total loss\n",
    "    g_loss = 0.1 * fft_loss + 0.0025* vgg_loss + adv_loss\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    # save batch losses\n",
    "    d_lossesb.append(-d_loss.item())\n",
    "    g_lossesb.append(g_loss.item())\n",
    "\n",
    "  avg_g_loss = sum(g_lossesb) / len(g_lossesb)\n",
    "  avg_d_loss = sum(d_lossesb) / len(d_lossesb)\n",
    "\n",
    "#   if avg_g_loss > 1.3 * best_avg_g_loss:\n",
    "#     raise Exception(\"Early Stopping\")\n",
    "\n",
    "  if avg_g_loss < best_avg_g_loss:\n",
    "    best_avg_g_loss = avg_g_loss\n",
    "\n",
    "  f, axarr = plt.subplots(nrows=1,ncols=3)\n",
    "  plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "  plt.sca(axarr[0]); \n",
    "  plt.imshow(unshaped_ts.cpu().detach().numpy(), cmap='gray')\n",
    "  plt.sca(axarr[1]); \n",
    "  plt.imshow(fake_images[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "  plt.sca(axarr[2]); \n",
    "  plt.imshow(tf[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "  plt.show()\n",
    "    \n",
    "  # save losses for the epoch\n",
    "  d_losses.append(-d_loss.item())\n",
    "  g_losses.append(g_loss.item())\n",
    "\n",
    "  # next do the validation loop\n",
    "  epoch_ssim_scores = []\n",
    "\n",
    "  # next do the validation loop\n",
    "  from random import randrange\n",
    "  chosen_index = randrange(0, len(dataloader2))\n",
    "  for index, (vf, vs) in enumerate(dataloader2):\n",
    "    if index == chosen_index:\n",
    "      vf = vf.float().to(device)\n",
    "      vs = vs.float().to(device)\n",
    "      valid = torch.ones(vf.shape[0]).float().to(device) # Labels\n",
    "      fake = torch.zeros(vf.shape[0]).float().to(device) # Labels\n",
    "        \n",
    "      vs = vs.reshape(vs.shape[0], (vs.shape[1] * vs.shape[2] * vs.shape[3]))\n",
    "      fake_images = G(vs)\n",
    "\n",
    "      #### Calculate Discriminator Validation Loss ####\n",
    "      # Total loss = mean of real loss + mean of fake loss (Wasserstein)\n",
    "      dv_loss = -torch.mean(D(vf)) + torch.mean(D(fake_images.detach()))\n",
    "\n",
    "      #### Calculate Generator Validation Loss ####\n",
    "      # Adversarial loss\n",
    "      adv_loss = torch.clamp(criterion_content(D(fake_images), valid), min=0, max=None)  # For L1 Loss (Best)\n",
    "      #adv_loss = torch.clamp(criterion_mse(D(fake_images), valid), min=0, max=None)  # For MSE Loss, from Erik Lindernoren SRGAN implementation\n",
    "\n",
    "      l1_loss = torch.clamp(criterion_content(fake_images, vf), min=0, max=None) # Pixel-by-pixel L1 loss (better than L2 loss)\n",
    "      mse_loss = torch.clamp(criterion_mse(fake_images, vf), min=0, max=None) # Pixel-by-pixel L2 loss\n",
    "\n",
    "      # FFT L1 loss - the pixel-by-pixel L1 loss in the frequency domain\n",
    "      fft_loss = torch.clamp(fft_l1(fake_images, vf), min=0, max=None)\n",
    "\n",
    "      # Perceptual loss (VGG)\n",
    "      # Since the VGG19 take in 3-channel images, we need to concatenate\n",
    "      fake_images_3ch = torch.cat((fake_images, fake_images, fake_images), dim=1)\n",
    "      real_images_3ch = torch.cat((vf, vf, vf), dim=1)\n",
    "      gen_features = feature_extractor(fake_images_3ch)\n",
    "      real_features = feature_extractor(real_images_3ch)\n",
    "\n",
    "      vgg_loss = torch.clamp(criterion_content(gen_features, real_features), min=0, max=None) # L1 loss\n",
    "      #vgg_loss = torch.clamp(criterion_mse(gen_features, real_features), min=0, max=None) # L2 loss\n",
    "\n",
    "      ssim_loss = torch.clamp(criterion_ssim(fake_images, vf), min=0, max=None)\n",
    "    \n",
    "      #gv_loss = adv_loss  # Model 1 & 2\n",
    "      #gv_loss = mse_loss + 6e-3 * vgg_loss + 1e-3 * adv_loss  # Model 3, Original SRGAN\n",
    "      #gv_loss = l1_loss + 6e-3 * vgg_loss + 1e-3 * adv_loss  # Model 4 & 14\n",
    "      #gv_loss = l1_loss + 6e-3 * vgg_loss + 1e-4 * adv_loss  # Model 15\n",
    "      gv_loss = 0.1 * fft_loss + 0.0025* vgg_loss + adv_loss # Model 16\n",
    "\n",
    "  # save validation losses for the epoch\n",
    "  dv_losses.append(-dv_loss.item())\n",
    "  gv_losses.append(gv_loss.item())\n",
    "  ssim_v_losses.append(ssim_loss.item())\n",
    "\n",
    "  ### print and save things ###\n",
    "  print(f\"Epoch: {epoch}, train d_loss: {avg_d_loss}, train g_loss: {avg_g_loss}, val d_loss: {dv_loss.item()}, val g_loss: {gv_loss.item()}\")\n",
    "#   writer.add_scalar(\"train d_loss\", d_loss, epoch)\n",
    "#   writer.add_scalar(\"train g_loss\", g_loss, epoch)\n",
    "#   writer.add_scalar(\"val d_loss\", dv_loss, epoch)\n",
    "#   writer.add_scalar(\"val g_loss\", gv_loss, epoch)\n",
    "#   writer.add_scalar(\"val ssim_loss\", ssim_loss, epoch)\n",
    "\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final mean ssim for the entire validation set\n",
    "# From https://ourcodeworld.com/articles/read/991/how-to-calculate-the-structural-similarity-index-ssim-between-two-images-with-python\n",
    "# Import the necessary packages\n",
    "from skimage.metrics import structural_similarity, mean_squared_error, peak_signal_noise_ratio\n",
    "\n",
    "dataloader3 = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False) # Validation datasets\n",
    "ssim_batches = []\n",
    "nmse_batches = []\n",
    "pnsr_batches = []\n",
    "\n",
    "for index, (vf, vs) in enumerate(dataloader3):\n",
    "  real_images = vf.float()\n",
    "  vs=vs.reshape(vs.shape[0], (vs.shape[1]*vs.shape[2]*vs.shape[3]))\n",
    "  fake_images = G(vs.float().to(device))\n",
    "  fake_images_np = fake_images.to('cpu').detach().numpy().squeeze(0).squeeze(0)\n",
    "  real_images_np = real_images.to('cpu').detach().numpy().squeeze(0).squeeze(0)\n",
    "\n",
    "  score = structural_similarity(fake_images_np, real_images_np, win_size=7, full=False, k1=0.01, k2=0.03)\n",
    "  ssim_batches.append(score)\n",
    "  nmse = mean_squared_error(fake_images_np, real_images_np) / np.linalg.norm(real_images_np)\n",
    "  nmse_batches.append(nmse)\n",
    "  pnsr = peak_signal_noise_ratio(fake_images_np, real_images_np, data_range=np.max(fake_images_np)-np.min(fake_images_np))\n",
    "  pnsr_batches.append(pnsr)\n",
    "\n",
    "ssim_final = np.asarray(ssim_batches)\n",
    "nmse_final = np.asarray(nmse_batches)\n",
    "pnsr_final = np.asarray(pnsr_batches)\n",
    "print('Validation Metrics:')\n",
    "print('Mean SSIM: ', np.mean(ssim_final), 'Min SSIM: ', np.min(ssim_final), 'Max SSIM: ', np.max(ssim_final))\n",
    "print('Mean NMSE: ', np.mean(nmse_final), 'Min NMSE: ', np.min(nmse_final), 'Max NMSE: ', np.max(nmse_final))\n",
    "print('Mean PNSR: ', np.mean(pnsr_final), 'Min PNSR: ', np.min(pnsr_final), 'Max PNSR: ', np.max(pnsr_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.plot(g_losses, label='train g_losses')\n",
    "plt.plot(gv_losses, label='val g_losses')\n",
    "plt.title('Generator Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(d_losses, label='train d_losses')\n",
    "plt.plot(dv_losses, label='val d_losses')\n",
    "plt.title('Discriminator Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ssim_v_losses, label='val ssim_losses')\n",
    "plt.title('Validation SSIM Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXtmVe5Mn2r7"
   },
   "source": [
    "## Save the models and reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tXcZDiqn5l2"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "torch.save(G, 'transformer_G_3.mod')\n",
    "# Discriminator\n",
    "torch.save(D, 'transformer_D_3.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFea5B47XKk0"
   },
   "source": [
    "## Show some generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show a subsampled, fake, and real images\n",
    "# #print(fake_images.shape)\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "# print(ts.shape)\n",
    "# ts=ts.reshape(ts.shape[0], -1)\n",
    "# generated_image = G(ts[0,:])\n",
    "# print(generated_image.shape)\n",
    "# ts=ts.reshape((ts.shape[0],1, (ts.shape[1]//128),(ts.shape[1]//128)))\n",
    "# print(ts.shape)\n",
    "# plt.imshow(ts[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(generated_image[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(tf[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLJOx21CbN7b"
   },
   "outputs": [],
   "source": [
    "# Show an example\n",
    "image_num = 24\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.imshow(val_ss[image_num,0], cmap='gray')\n",
    "plt.show()\n",
    "vs = vs.reshape(vs.shape[0], -1)\n",
    "generated = G(vs.float().to(device)).cpu().detach().numpy()\n",
    "plt.imshow(generated[0,0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(val_full[image_num,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Workflow_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
